{
  "report_timestamp": "2025-10-02T20:18:11.020897",
  "api_url": "https://rickandmortyapi.com/graphql",
  "summary": {
    "total_comparisons": 10,
    "agreement_rate": 0.0,
    "simple_detections": 1,
    "llm_detections": 3,
    "consensus_breakdown": {
      "no_vulnerabilities": 7,
      "llm_only": 2,
      "both_detected": 1
    },
    "detection_ratio": 3.0,
    "potential_analysis": {
      "potential_agreement_rate": 0.0,
      "simple_potential_detections": 0,
      "llm_potential_detections": 2,
      "potential_consensus_breakdown": {
        "no_potential": 8,
        "llm_potential_only": 2
      },
      "potential_detection_ratio": Infinity
    }
  },
  "detailed_comparisons": [
    {
      "timestamp": "2025-10-02T20:16:31.805285",
      "node_name": "charactersByIds_response_2",
      "response_status": 200,
      "response_body": {
        "data": {
          "charactersByIds": null
        },
        "errors": [
          {
            "message": "500: Internal Server Error",
            "locations": [
              {
                "line": 2,
                "column": 3
              }
            ],
            "path": [
              "charactersByIds"
            ],
            "extensions": {
              "code": "INTERNAL_SERVER_ERROR",
              "response": {
                "url": "http://localhost:8080/api/character/1,2%27%20OR%20%271%27=%271",
                "status": 500,
                "statusText": "Internal Server Error",
                "body": {
                  "error": "Cast to Number failed for value \"NaN\" at path \"id\" for model \"Character\""
                }
              }
            }
          }
        ]
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-02T20:16:39.914371",
      "node_name": "charactersByIds_response_3",
      "response_status": 200,
      "response_body": {
        "data": {
          "charactersByIds": []
        }
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-02T20:16:53.554330",
      "node_name": "charactersByIds_response_4",
      "response_status": 200,
      "response_body": {
        "data": {
          "charactersByIds": null
        },
        "errors": [
          {
            "message": "Cannot read property 'response' of undefined",
            "locations": [
              {
                "line": 2,
                "column": 3
              }
            ],
            "path": [
              "charactersByIds"
            ],
            "extensions": {
              "code": "INTERNAL_SERVER_ERROR"
            }
          }
        ]
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-02T20:17:04.789965",
      "node_name": "charactersByIds_response_5",
      "response_status": 200,
      "response_body": {
        "data": {
          "charactersByIds": []
        }
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-02T20:17:17.262970",
      "node_name": "charactersByIds_response_6",
      "response_status": 200,
      "response_body": {
        "data": {
          "charactersByIds": []
        }
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-02T20:17:31.375134",
      "node_name": "charactersByIds_response_7",
      "response_status": 200,
      "response_body": {
        "data": {
          "charactersByIds": null
        },
        "errors": [
          {
            "message": "500: Internal Server Error",
            "locations": [
              {
                "line": 2,
                "column": 3
              }
            ],
            "path": [
              "charactersByIds"
            ],
            "extensions": {
              "code": "INTERNAL_SERVER_ERROR",
              "response": {
                "url": "http://localhost:8080/api/character/%3Cimg%20src=x%20onerror=alert(%27xss%27)%3E",
                "status": 500,
                "statusText": "Internal Server Error",
                "body": {
                  "error": "Hey! you must provide an id"
                }
              }
            }
          }
        ]
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [
          {
            "detection_name": "LLM: html_injection",
            "detection": "potential",
            "category": "Injection Attacks",
            "description": "The GraphQL error response reflects user-supplied input containing an HTML <img> tag (percent-encoded) inside the returned JSON (errors.extensions.response.url). This is a clear reflection of markup in server output. If a client or UI renders that URL value into the DOM without proper escaping or innerText usage, the injected tag could be interpreted by the browser and trigger script execution (reflected XSS / HTML injection). The response currently encodes the characters (percent-encoded), but the presence of the raw injected payload in the response metadata is unsafe and constitutes an HTML injection vector unless the client treats it as plain text or the server sanitizes/escapes it before returning. Recommend treating all user-controlled values as untrusted, escaping/encoding before inclusion in HTML, and avoiding echoing raw input in error messages.",
            "evidence": "errors[0].extensions.response.url = \"http://localhost:8080/api/character/%3Cimg%20src=x%20onerror=alert(%27xss%27)%3E\"",
            "confidence": 0.75,
            "llm_analysis": {
              "is_vulnerable": true,
              "vulnerability_type": "html_injection",
              "severity": "MEDIUM",
              "confidence": 0.75,
              "evidence": "errors[0].extensions.response.url = \"http://localhost:8080/api/character/%3Cimg%20src=x%20onerror=alert(%27xss%27)%3E\"",
              "explanation": "The GraphQL error response reflects user-supplied input containing an HTML <img> tag (percent-encoded) inside the returned JSON (errors.extensions.response.url). This is a clear reflection of markup in server output. If a client or UI renders that URL value into the DOM without proper escaping or innerText usage, the injected tag could be interpreted by the browser and trigger script execution (reflected XSS / HTML injection). The response currently encodes the characters (percent-encoded), but the presence of the raw injected payload in the response metadata is unsafe and constitutes an HTML injection vector unless the client treats it as plain text or the server sanitizes/escapes it before returning. Recommend treating all user-controlled values as untrusted, escaping/encoding before inclusion in HTML, and avoiding echoing raw input in error messages."
            }
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 1,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": [
          "Injection Attacks"
        ]
      },
      "comparison": {
        "total_categories": [
          "Injection Attacks"
        ],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [
          "Injection Attacks"
        ],
        "agreement_score": 0.0,
        "consensus": "llm_only",
        "potential_analysis": {
          "total_potential_categories": [
            "Injection Attacks"
          ],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [
            "Injection Attacks"
          ],
          "potential_agreement_score": 0.0,
          "potential_consensus": "llm_potential_only"
        }
      }
    },
    {
      "timestamp": "2025-10-02T20:17:40.428081",
      "node_name": "charactersByIds_response_8",
      "response_status": null,
      "response_body": null,
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-02T20:17:46.761125",
      "node_name": "charactersByIds_response_9",
      "response_status": null,
      "response_body": null,
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-02T20:17:55.488946",
      "node_name": "charactersByIds_response_10",
      "response_status": 200,
      "response_body": {
        "data": {
          "charactersByIds": [
            {
              "id": "1",
              "name": "Rick Sanchez"
            }
          ],
          "__type": {
            "name": "Character",
            "fields": [
              {
                "name": "id",
                "type": {
                  "name": "ID"
                }
              },
              {
                "name": "name",
                "type": {
                  "name": "String"
                }
              },
              {
                "name": "status",
                "type": {
                  "name": "String"
                }
              },
              {
                "name": "species",
                "type": {
                  "name": "String"
                }
              },
              {
                "name": "type",
                "type": {
                  "name": "String"
                }
              },
              {
                "name": "gender",
                "type": {
                  "name": "String"
                }
              },
              {
                "name": "origin",
                "type": {
                  "name": "Location"
                }
              },
              {
                "name": "location",
                "type": {
                  "name": "Location"
                }
              },
              {
                "name": "image",
                "type": {
                  "name": "String"
                }
              },
              {
                "name": "episode",
                "type": {
                  "name": null
                }
              },
              {
                "name": "created",
                "type": {
                  "name": "String"
                }
              }
            ]
          }
        }
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [
          {
            "detection_name": "LLM: Introspection",
            "detection": "potential",
            "category": "Injection Attacks",
            "description": "The response contains an explicit GraphQL introspection payload: the __type object with the 'Character' type and a full list of its fields (id, name, status, species, type, gender, origin, location, image, episode, created). This is a clear sign that GraphQL introspection is enabled and returning schema metadata. Exposed schema information can help attackers discover available types and fields, locate potentially sensitive data, and craft targeted queries or attacks (enumeration, data extraction, privilege escalation). Recommend disabling introspection in production or restricting it to authenticated/authorized users, and reviewing exposed fields for sensitive information.",
            "evidence": "\"__type\": { \"name\": \"Character\", \"fields\": [ { \"name\": \"id\", ... }, { \"name\": \"name\", ... }, { \"name\": \"status\", ... } ] }",
            "confidence": 0.95,
            "llm_analysis": {
              "is_vulnerable": true,
              "vulnerability_type": "Introspection",
              "severity": "MEDIUM",
              "confidence": 0.95,
              "evidence": "\"__type\": { \"name\": \"Character\", \"fields\": [ { \"name\": \"id\", ... }, { \"name\": \"name\", ... }, { \"name\": \"status\", ... } ] }",
              "explanation": "The response contains an explicit GraphQL introspection payload: the __type object with the 'Character' type and a full list of its fields (id, name, status, species, type, gender, origin, location, image, episode, created). This is a clear sign that GraphQL introspection is enabled and returning schema metadata. Exposed schema information can help attackers discover available types and fields, locate potentially sensitive data, and craft targeted queries or attacks (enumeration, data extraction, privilege escalation). Recommend disabling introspection in production or restricting it to authenticated/authorized users, and reviewing exposed fields for sensitive information."
            }
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 1,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": [
          "Injection Attacks"
        ]
      },
      "comparison": {
        "total_categories": [
          "Injection Attacks"
        ],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [
          "Injection Attacks"
        ],
        "agreement_score": 0.0,
        "consensus": "llm_only",
        "potential_analysis": {
          "total_potential_categories": [
            "Injection Attacks"
          ],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [
            "Injection Attacks"
          ],
          "potential_agreement_score": 0.0,
          "potential_consensus": "llm_potential_only"
        }
      }
    },
    {
      "timestamp": "2025-10-02T20:18:11.017629",
      "node_name": "charactersByIds_response_11",
      "response_status": 400,
      "response_body": {
        "errors": [
          {
            "message": "Cannot query field \"nam\" on type \"Character\". Did you mean \"name\"?",
            "locations": [
              {
                "line": 4,
                "column": 5
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          },
          {
            "message": "Cannot query field \"statuss\" on type \"Character\". Did you mean \"status\"?",
            "locations": [
              {
                "line": 5,
                "column": 5
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          }
        ]
      },
      "simple_detector": {
        "results": [
          {
            "detection_name": "Field Suggestions Enabled",
            "detection": "vulnerable",
            "category": "Information Disclosure",
            "description": "Field suggestions enabled - potential information disclosure",
            "evidence": "Found \"did you mean\" in error message - field suggestions are enabled"
          }
        ],
        "count": 1,
        "vulnerable_count": 1,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [
          "Information Disclosure"
        ],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [
          {
            "detection_name": "LLM: field_suggestions",
            "detection": "safe",
            "category": "Unknown",
            "description": "The GraphQL error responses include field suggestion text that discloses valid schema field names (e.g., suggesting \"name\" and \"status\"). This is an information disclosure vulnerability: it leaks parts of the GraphQL schema which can aid an attacker in enumerating and crafting targeted queries. The issue is evident from the error messages returned in the 400 response. Recommended mitigations: disable or sanitize suggestion messages in production (turn off detailed GraphQL validation suggestions/debugging), return generic validation errors, and enforce proper authentication/authorization to limit schema exposure.",
            "evidence": "Error messages: \"Cannot query field \\\"nam\\\" on type \\\"Character\\\". Did you mean \\\"name\\\"?\" and \"Cannot query field \\\"statuss\\\" on type \\\"Character\\\". Did you mean \\\"status\\\"?\"",
            "confidence": 0.95,
            "llm_analysis": {
              "is_vulnerable": true,
              "vulnerability_type": "field_suggestions",
              "severity": "LOW",
              "confidence": 0.95,
              "evidence": "Error messages: \"Cannot query field \\\"nam\\\" on type \\\"Character\\\". Did you mean \\\"name\\\"?\" and \"Cannot query field \\\"statuss\\\" on type \\\"Character\\\". Did you mean \\\"status\\\"?\"",
              "explanation": "The GraphQL error responses include field suggestion text that discloses valid schema field names (e.g., suggesting \"name\" and \"status\"). This is an information disclosure vulnerability: it leaks parts of the GraphQL schema which can aid an attacker in enumerating and crafting targeted queries. The issue is evident from the error messages returned in the 400 response. Recommended mitigations: disable or sanitize suggestion messages in production (turn off detailed GraphQL validation suggestions/debugging), return generic validation errors, and enforce proper authentication/authorization to limit schema exposure."
            }
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 1,
        "categories": [
          "Unknown"
        ],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [
          "Unknown",
          "Information Disclosure"
        ],
        "common_categories": [],
        "simple_unique_categories": [
          "Information Disclosure"
        ],
        "llm_unique_categories": [
          "Unknown"
        ],
        "agreement_score": 0.0,
        "consensus": "both_detected",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    }
  ],
  "detector_analysis": {
    "simple_detector": {
      "description": "Rule-based detector using predefined patterns",
      "strengths": [
        "Fast execution",
        "Consistent results",
        "Low resource usage"
      ],
      "weaknesses": [
        "Limited to known patterns",
        "May miss novel attacks",
        "False negatives possible"
      ]
    },
    "llm_detector": {
      "description": "AI-powered detector using language model analysis",
      "strengths": [
        "Contextual understanding",
        "Novel attack detection",
        "Detailed analysis"
      ],
      "weaknesses": [
        "Slower execution",
        "Resource intensive",
        "Potential false positives"
      ]
    }
  }
}