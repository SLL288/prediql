{
  "report_timestamp": "2025-10-05T14:26:13.626545",
  "api_url": "https://graphqlzero.almansi.me/api",
  "summary": {
    "total_comparisons": 10,
    "agreement_rate": 0.1,
    "simple_detections": 2,
    "llm_detections": 4,
    "consensus_breakdown": {
      "no_vulnerabilities": 6,
      "both_detected": 2,
      "llm_only": 2
    },
    "detection_ratio": 2.0,
    "potential_analysis": {
      "potential_agreement_rate": 0.0,
      "simple_potential_detections": 1,
      "llm_potential_detections": 1,
      "potential_consensus_breakdown": {
        "no_potential": 8,
        "simple_potential_only": 1,
        "llm_potential_only": 1
      },
      "potential_detection_ratio": 1.0
    }
  },
  "detailed_comparisons": [
    {
      "timestamp": "2025-10-05T14:22:58.345046",
      "node_name": "updateAlbum_response_2",
      "response_status": 200,
      "response_body": {
        "data": {
          "updateAlbum": {
            "id": null,
            "title": "Hacked"
          }
        }
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-05T14:23:14.823276",
      "node_name": "updateAlbum_response_3",
      "response_status": 200,
      "response_body": {
        "data": {
          "updateAlbum": {
            "id": "2",
            "title": "<img src=x onerror=alert(document.domain)>"
          }
        }
      },
      "simple_detector": {
        "results": [
          {
            "detection_name": "Cross-Site Scripting (XSS) Injection",
            "detection": "potential",
            "category": "Injection Attacks",
            "description": "XSS injection vulnerability detected - malicious script patterns found in response",
            "evidence": "Response status: 200, XSS patterns detected in response data"
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 1,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": [
          "Injection Attacks"
        ]
      },
      "llm_detector": {
        "results": [
          {
            "detection_name": "LLM: xss_injection",
            "detection": "vulnerable",
            "category": "Injection Attacks",
            "description": "The response returns unencoded/sanitized HTML payload in the title field, containing a classic XSS vector (<img> with onerror event handler). If this value is rendered in a browser context without output encoding, it would execute arbitrary JavaScript (alert(document.domain)). This indicates insufficient input validation/sanitization in the GraphQL endpoint, making stored XSS possible.",
            "evidence": "title: '<img src=x onerror=alert(document.domain)>'",
            "confidence": 0.95,
            "llm_analysis": {
              "is_vulnerable": true,
              "vulnerability_type": "xss_injection",
              "severity": "CRITICAL",
              "confidence": 0.95,
              "evidence": "title: '<img src=x onerror=alert(document.domain)>'",
              "explanation": "The response returns unencoded/sanitized HTML payload in the title field, containing a classic XSS vector (<img> with onerror event handler). If this value is rendered in a browser context without output encoding, it would execute arbitrary JavaScript (alert(document.domain)). This indicates insufficient input validation/sanitization in the GraphQL endpoint, making stored XSS possible."
            }
          }
        ],
        "count": 1,
        "vulnerable_count": 1,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [
          "Injection Attacks"
        ],
        "common_categories": [
          "Injection Attacks"
        ],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 1.0,
        "consensus": "both_detected",
        "potential_analysis": {
          "total_potential_categories": [
            "Injection Attacks"
          ],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [
            "Injection Attacks"
          ],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "simple_potential_only"
        }
      }
    },
    {
      "timestamp": "2025-10-05T14:23:32.371229",
      "node_name": "updateAlbum_response_4",
      "response_status": 400,
      "response_body": {
        "errors": [
          {
            "message": "Field \"photos\" is not defined by type \"UpdateAlbumInput\".",
            "locations": [
              {
                "line": 3,
                "column": 33
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          }
        ]
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-05T14:23:38.766531",
      "node_name": "updateAlbum_response_5",
      "response_status": 200,
      "response_body": {
        "data": {
          "updateAlbum": {
            "id": "4",
            "title": "test $(rm -rf /)"
          }
        }
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [
          {
            "detection_name": "LLM: os_command_injection",
            "detection": "vulnerable",
            "category": "Injection Attacks",
            "description": "The response includes a payload containing '$(rm -rf /)', a command substitution syntax used in Unix shells. This suggests the application may be passing unsanitized user input directly to a system shell, allowing arbitrary command execution (e.g., 'rm -rf /' would delete all files recursively). While this does not confirm execution occurred, the presence of shell-specific syntax in a GraphQL mutation parameter strongly indicates an OS command injection vulnerability.",
            "evidence": "title: 'test $(rm -rf /)'",
            "confidence": 0.85,
            "llm_analysis": {
              "is_vulnerable": true,
              "vulnerability_type": "os_command_injection",
              "severity": "CRITICAL",
              "confidence": 0.85,
              "evidence": "title: 'test $(rm -rf /)'",
              "explanation": "The response includes a payload containing '$(rm -rf /)', a command substitution syntax used in Unix shells. This suggests the application may be passing unsanitized user input directly to a system shell, allowing arbitrary command execution (e.g., 'rm -rf /' would delete all files recursively). While this does not confirm execution occurred, the presence of shell-specific syntax in a GraphQL mutation parameter strongly indicates an OS command injection vulnerability."
            }
          }
        ],
        "count": 1,
        "vulnerable_count": 1,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [
          "Injection Attacks"
        ],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [
          "Injection Attacks"
        ],
        "agreement_score": 0.0,
        "consensus": "llm_only",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-05T14:23:46.369751",
      "node_name": "updateAlbum_response_6",
      "response_status": 400,
      "response_body": {
        "errors": [
          {
            "message": "Field \"photos\" is not defined by type \"UpdateAlbumInput\".",
            "locations": [
              {
                "line": 3,
                "column": 33
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          }
        ]
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-05T14:23:53.586723",
      "node_name": "updateAlbum_response_7",
      "response_status": 200,
      "response_body": {
        "data": {
          "updateAlbum": {
            "id": "6",
            "title": "<div style='background:url(javascript:alert(1))'>"
          }
        }
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [
          {
            "detection_name": "LLM: HTML Injection",
            "detection": "vulnerable",
            "category": "Injection Attacks",
            "description": "The response contains unsanitized user-controlled HTML markup (`<div>` with inline CSS containing a JavaScript URI scheme). This indicates HTML injection vulnerability as the payload would execute JavaScript if rendered in a browser context. While modern browsers may block `javascript:` URIs in CSS background properties, the presence of raw HTML in the response demonstrates insufficient output encoding.",
            "evidence": "title: \"<div style='background:url(javascript:alert(1))'>\"",
            "confidence": 0.95,
            "llm_analysis": {
              "is_vulnerable": true,
              "vulnerability_type": "HTML Injection",
              "severity": "HIGH",
              "confidence": 0.95,
              "evidence": "title: \"<div style='background:url(javascript:alert(1))'>\"",
              "explanation": "The response contains unsanitized user-controlled HTML markup (`<div>` with inline CSS containing a JavaScript URI scheme). This indicates HTML injection vulnerability as the payload would execute JavaScript if rendered in a browser context. While modern browsers may block `javascript:` URIs in CSS background properties, the presence of raw HTML in the response demonstrates insufficient output encoding."
            }
          }
        ],
        "count": 1,
        "vulnerable_count": 1,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [
          "Injection Attacks"
        ],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [
          "Injection Attacks"
        ],
        "agreement_score": 0.0,
        "consensus": "llm_only",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-05T14:24:06.038455",
      "node_name": "updateAlbum_response_8",
      "response_status": null,
      "response_body": null,
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-05T14:25:45.609475",
      "node_name": "updateAlbum_response_9",
      "response_status": null,
      "response_body": null,
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-05T14:25:50.663198",
      "node_name": "updateAlbum_response_10",
      "response_status": 400,
      "response_body": {
        "errors": [
          {
            "message": "Cannot query field \"updateAlbum\" on type \"Query\".",
            "locations": [
              {
                "line": 3,
                "column": 3
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          }
        ]
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-05T14:26:13.621968",
      "node_name": "updateAlbum_response_11",
      "response_status": 400,
      "response_body": {
        "errors": [
          {
            "message": "Unknown argument \"intput\" on field \"Mutation.updateAlbum\". Did you mean \"input\"?",
            "locations": [
              {
                "line": 3,
                "column": 24
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          },
          {
            "message": "Cannot query field \"ud\" on type \"Album\". Did you mean \"id\"?",
            "locations": [
              {
                "line": 4,
                "column": 5
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          },
          {
            "message": "Field \"updateAlbum\" argument \"input\" of type \"UpdateAlbumInput!\" is required, but it was not provided.",
            "locations": [
              {
                "line": 3,
                "column": 3
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          }
        ]
      },
      "simple_detector": {
        "results": [
          {
            "detection_name": "Field Suggestions Enabled",
            "detection": "vulnerable",
            "category": "Information Disclosure",
            "description": "Field suggestions enabled - potential information disclosure",
            "evidence": "Found \"did you mean\" in error message - field suggestions are enabled"
          }
        ],
        "count": 1,
        "vulnerable_count": 1,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [
          "Information Disclosure"
        ],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [
          {
            "detection_name": "LLM: Information Disclosure",
            "detection": "potential",
            "category": "Injection Attacks",
            "description": "The server returns field/argument suggestions (\"Did you mean\") in validation errors, which reveals valid schema elements. While not critical, this information disclosure helps attackers probe the API by exposing valid field/argument names. In GraphQL, this could facilitate schema harvesting even when introspection is disabled. Production environments should suppress such suggestions to prevent reconnaissance.",
            "evidence": "\"Did you mean 'input'?\" and \"Did you mean 'id'?\" suggestions in error messages",
            "confidence": 0.95,
            "llm_analysis": {
              "is_vulnerable": true,
              "vulnerability_type": "Information Disclosure",
              "severity": "MEDIUM",
              "confidence": 0.95,
              "evidence": "\"Did you mean 'input'?\" and \"Did you mean 'id'?\" suggestions in error messages",
              "explanation": "The server returns field/argument suggestions (\"Did you mean\") in validation errors, which reveals valid schema elements. While not critical, this information disclosure helps attackers probe the API by exposing valid field/argument names. In GraphQL, this could facilitate schema harvesting even when introspection is disabled. Production environments should suppress such suggestions to prevent reconnaissance."
            }
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 1,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": [
          "Injection Attacks"
        ]
      },
      "comparison": {
        "total_categories": [
          "Information Disclosure",
          "Injection Attacks"
        ],
        "common_categories": [],
        "simple_unique_categories": [
          "Information Disclosure"
        ],
        "llm_unique_categories": [
          "Injection Attacks"
        ],
        "agreement_score": 0.0,
        "consensus": "both_detected",
        "potential_analysis": {
          "total_potential_categories": [
            "Injection Attacks"
          ],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [
            "Injection Attacks"
          ],
          "potential_agreement_score": 0.0,
          "potential_consensus": "llm_potential_only"
        }
      }
    }
  ],
  "detector_analysis": {
    "simple_detector": {
      "description": "Rule-based detector using predefined patterns",
      "strengths": [
        "Fast execution",
        "Consistent results",
        "Low resource usage"
      ],
      "weaknesses": [
        "Limited to known patterns",
        "May miss novel attacks",
        "False negatives possible"
      ]
    },
    "llm_detector": {
      "description": "AI-powered detector using language model analysis",
      "strengths": [
        "Contextual understanding",
        "Novel attack detection",
        "Detailed analysis"
      ],
      "weaknesses": [
        "Slower execution",
        "Resource intensive",
        "Potential false positives"
      ]
    }
  }
}