{
  "report_timestamp": "2025-09-23T20:22:10.848324",
  "api_url": "https://graphqlzero.almansi.me/api",
  "summary": {
    "total_comparisons": 10,
    "agreement_rate": 0.2,
    "simple_detections": 6,
    "llm_detections": 3,
    "consensus_breakdown": {
      "simple_only": 3,
      "both_detected": 3,
      "no_vulnerabilities": 4
    },
    "detection_ratio": 0.5,
    "potential_analysis": {
      "potential_agreement_rate": 0.0,
      "simple_potential_detections": 5,
      "llm_potential_detections": 0,
      "potential_consensus_breakdown": {
        "simple_potential_only": 5,
        "no_potential": 5
      },
      "potential_detection_ratio": 0.0
    }
  },
  "detailed_comparisons": [
    {
      "timestamp": "2025-09-23T20:19:51.441706",
      "node_name": "updateAlbum_response_2",
      "response_status": 200,
      "response_body": {
        "data": {
          "updateAlbum": {
            "id": "1",
            "title": "Robert'); DROP TABLE albums; --",
            "user": {
              "id": "1",
              "name": "Leanne Graham"
            }
          }
        }
      },
      "simple_detector": {
        "results": [
          {
            "detection_name": "SQL Injection (SQLi) Injection",
            "detection": "potential",
            "category": "Injection Attacks",
            "description": "SQL injection vulnerability detected - malicious SQL patterns found in response",
            "evidence": "Response status: 200, SQL patterns detected in response"
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 1,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": [
          "Injection Attacks"
        ]
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [
          "Injection Attacks"
        ],
        "common_categories": [],
        "simple_unique_categories": [
          "Injection Attacks"
        ],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "simple_only",
        "potential_analysis": {
          "total_potential_categories": [
            "Injection Attacks"
          ],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [
            "Injection Attacks"
          ],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "simple_potential_only"
        }
      }
    },
    {
      "timestamp": "2025-09-23T20:20:06.660258",
      "node_name": "updateAlbum_response_3",
      "response_status": 200,
      "response_body": {
        "data": {
          "updateAlbum": {
            "id": "2",
            "title": "<script>alert('xss')</script>",
            "user": {
              "id": "1",
              "name": "Leanne Graham"
            }
          }
        }
      },
      "simple_detector": {
        "results": [
          {
            "detection_name": "Cross-Site Scripting (XSS) Injection",
            "detection": "potential",
            "category": "Injection Attacks",
            "description": "XSS injection vulnerability detected - malicious script patterns found in response",
            "evidence": "Response status: 200, XSS patterns detected in response data"
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 1,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": [
          "Injection Attacks"
        ]
      },
      "llm_detector": {
        "results": [
          {
            "detection_name": "LLM: xss_injection",
            "detection": "vulnerable",
            "category": "Injection Attacks",
            "description": "The GraphQL response returns unescaped HTML/JavaScript in the title field: the value of data.updateAlbum.title is \"<script>alert('xss')</script>\". This indicates user-controlled input was persisted/reflected by the API without sanitization or encoding. If a client renders this value into a web page DOM without proper output encoding or sanitization, it would allow execution of arbitrary JavaScript (stored/reflected XSS). While the API response alone does not prove client-side rendering is vulnerable, returning raw script tags from the API is a clear unsafe behavior and constitutes a high-risk XSS vector. Recommend validating/encoding/sanitizing input on write, performing context-aware output encoding on render, and implementing CSP to mitigate impact.",
            "evidence": "\"data.updateAlbum.title\": \"<script>alert('xss')</script>\"",
            "confidence": 0.9,
            "llm_analysis": {
              "is_vulnerable": true,
              "vulnerability_type": "xss_injection",
              "severity": "HIGH",
              "confidence": 0.9,
              "evidence": "\"data.updateAlbum.title\": \"<script>alert('xss')</script>\"",
              "explanation": "The GraphQL response returns unescaped HTML/JavaScript in the title field: the value of data.updateAlbum.title is \"<script>alert('xss')</script>\". This indicates user-controlled input was persisted/reflected by the API without sanitization or encoding. If a client renders this value into a web page DOM without proper output encoding or sanitization, it would allow execution of arbitrary JavaScript (stored/reflected XSS). While the API response alone does not prove client-side rendering is vulnerable, returning raw script tags from the API is a clear unsafe behavior and constitutes a high-risk XSS vector. Recommend validating/encoding/sanitizing input on write, performing context-aware output encoding on render, and implementing CSP to mitigate impact."
            }
          }
        ],
        "count": 1,
        "vulnerable_count": 1,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [
          "Injection Attacks"
        ],
        "common_categories": [
          "Injection Attacks"
        ],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 1.0,
        "consensus": "both_detected",
        "potential_analysis": {
          "total_potential_categories": [
            "Injection Attacks"
          ],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [
            "Injection Attacks"
          ],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "simple_potential_only"
        }
      }
    },
    {
      "timestamp": "2025-09-23T20:20:31.142131",
      "node_name": "updateAlbum_response_4",
      "response_status": 200,
      "response_body": {
        "data": {
          "updateAlbum": {
            "id": "3",
            "title": "FetchCover:http://169.254.169.254/latest/meta-data/",
            "user": {
              "id": "1",
              "name": "Leanne Graham"
            }
          }
        }
      },
      "simple_detector": {
        "results": [
          {
            "detection_name": "Server-Side Request Forgery (SSRF) Injection",
            "detection": "potential",
            "category": "Injection Attacks",
            "description": "SSRF injection vulnerability detected - internal network access patterns found",
            "evidence": "Response status: 200, SSRF keywords detected in response"
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 1,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": [
          "Injection Attacks"
        ]
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [
          "Injection Attacks"
        ],
        "common_categories": [],
        "simple_unique_categories": [
          "Injection Attacks"
        ],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "simple_only",
        "potential_analysis": {
          "total_potential_categories": [
            "Injection Attacks"
          ],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [
            "Injection Attacks"
          ],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "simple_potential_only"
        }
      }
    },
    {
      "timestamp": "2025-09-23T20:20:46.967022",
      "node_name": "updateAlbum_response_5",
      "response_status": 200,
      "response_body": {
        "data": {
          "updateAlbum": {
            "id": "4",
            "title": "album_name; cat /etc/passwd",
            "user": {
              "id": "1",
              "name": "Leanne Graham"
            }
          }
        }
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-09-23T20:21:03.100187",
      "node_name": "updateAlbum_response_6",
      "response_status": 200,
      "response_body": {
        "data": {
          "updateAlbum": {
            "id": "5",
            "title": "../../etc/passwd",
            "user": {
              "id": "1",
              "name": "Leanne Graham"
            }
          }
        }
      },
      "simple_detector": {
        "results": [
          {
            "detection_name": "Path Injection",
            "detection": "potential",
            "category": "Injection Attacks",
            "description": "Path injection vulnerability detected - file system access patterns found",
            "evidence": "Response status: 200, Path injection patterns detected in response"
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 1,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": [
          "Injection Attacks"
        ]
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [
          "Injection Attacks"
        ],
        "common_categories": [],
        "simple_unique_categories": [
          "Injection Attacks"
        ],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "simple_only",
        "potential_analysis": {
          "total_potential_categories": [
            "Injection Attacks"
          ],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [
            "Injection Attacks"
          ],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "simple_potential_only"
        }
      }
    },
    {
      "timestamp": "2025-09-23T20:21:15.072789",
      "node_name": "updateAlbum_response_7",
      "response_status": 200,
      "response_body": {
        "data": {
          "updateAlbum": {
            "id": "6",
            "title": "<img src=\"x\" onerror=\"alert('html')\" />",
            "user": {
              "id": "1",
              "name": "Leanne Graham"
            }
          }
        }
      },
      "simple_detector": {
        "results": [
          {
            "detection_name": "HTML Injection",
            "detection": "potential",
            "category": "Injection Attacks",
            "description": "HTML injection vulnerability detected - HTML markup found in response",
            "evidence": "Response status: 200, HTML patterns detected in response data"
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 1,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": [
          "Injection Attacks"
        ]
      },
      "llm_detector": {
        "results": [
          {
            "detection_name": "LLM: html_injection",
            "detection": "vulnerable",
            "category": "Injection Attacks",
            "description": "The GraphQL response returns a field (title) containing raw HTML markup with an active event handler (an <img> tag with onerror JavaScript). This is clear evidence that HTML/JS was accepted and reflected by the API. If a client renders this value into a page without proper output encoding or sanitization (e.g., using innerHTML), it would execute the onerror handler and result in XSS. Mitigations: validate or strip HTML on input, encode/escape HTML on output, or use a safe rendering method (treat values as plain text). To fully confirm exploitability you must verify how the client renders this field, but the presence of the unescaped payload in the API response demonstrates an HTML injection risk.",
            "evidence": "\"title\": \"<img src=\\\"x\\\" onerror=\\\"alert('html')\\\" />\"",
            "confidence": 0.8,
            "llm_analysis": {
              "is_vulnerable": true,
              "vulnerability_type": "html_injection",
              "severity": "HIGH",
              "confidence": 0.8,
              "evidence": "\"title\": \"<img src=\\\"x\\\" onerror=\\\"alert('html')\\\" />\"",
              "explanation": "The GraphQL response returns a field (title) containing raw HTML markup with an active event handler (an <img> tag with onerror JavaScript). This is clear evidence that HTML/JS was accepted and reflected by the API. If a client renders this value into a page without proper output encoding or sanitization (e.g., using innerHTML), it would execute the onerror handler and result in XSS. Mitigations: validate or strip HTML on input, encode/escape HTML on output, or use a safe rendering method (treat values as plain text). To fully confirm exploitability you must verify how the client renders this field, but the presence of the unescaped payload in the API response demonstrates an HTML injection risk."
            }
          }
        ],
        "count": 1,
        "vulnerable_count": 1,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [
          "Injection Attacks"
        ],
        "common_categories": [
          "Injection Attacks"
        ],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 1.0,
        "consensus": "both_detected",
        "potential_analysis": {
          "total_potential_categories": [
            "Injection Attacks"
          ],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [
            "Injection Attacks"
          ],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "simple_potential_only"
        }
      }
    },
    {
      "timestamp": "2025-09-23T20:21:24.458776",
      "node_name": "updateAlbum_response_8",
      "response_status": null,
      "response_body": null,
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-09-23T20:21:33.851554",
      "node_name": "updateAlbum_response_9",
      "response_status": null,
      "response_body": null,
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-09-23T20:21:56.652812",
      "node_name": "updateAlbum_response_10",
      "response_status": 400,
      "response_body": {
        "errors": [
          {
            "message": "Cannot query field \"updateAlbum\" on type \"Query\".",
            "locations": [
              {
                "line": 3,
                "column": 3
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          }
        ]
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-09-23T20:22:10.844954",
      "node_name": "updateAlbum_response_11",
      "response_status": 400,
      "response_body": {
        "errors": [
          {
            "message": "Cannot query field \"titel\" on type \"Album\". Did you mean \"title\"?",
            "locations": [
              {
                "line": 5,
                "column": 5
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          }
        ]
      },
      "simple_detector": {
        "results": [
          {
            "detection_name": "Field Suggestions Enabled",
            "detection": "vulnerable",
            "category": "Information Disclosure",
            "description": "Field suggestions enabled - potential information disclosure",
            "evidence": "Found \"did you mean\" in error message - field suggestions are enabled"
          }
        ],
        "count": 1,
        "vulnerable_count": 1,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [
          "Information Disclosure"
        ],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [
          {
            "detection_name": "LLM: field_suggestions",
            "detection": "safe",
            "category": "Unknown",
            "description": "The GraphQL response returns a helpful suggestion that reveals the correct field name on the Album type: \"Did you mean \\\"title\\\"?\". This is a field_suggestions information-disclosure issue: an attacker can use such suggestions to enumerate schema details (field names and types) without authentication, making it easier to craft further targeted queries or perform automated schema discovery. The status code and validation error further confirm the server is returning detailed GraphQL validation feedback.\n\nAlthough this does not directly execute code or expose sensitive data, leaking schema details increases reconnaissance capability and should be treated as information disclosure. Mitigation: disable or suppress field-suggestion messages in production (turn off debug/verbose validation messages), return generic validation errors, and implement rate-limiting and authentication to reduce automated schema enumeration risk.",
            "evidence": "Error message: \"Cannot query field \\\"titel\\\" on type \\\"Album\\\". Did you mean \\\"title\\\"?\" (extensions.code: \"GRAPHQL_VALIDATION_FAILED\")",
            "confidence": 0.95,
            "llm_analysis": {
              "is_vulnerable": true,
              "vulnerability_type": "field_suggestions",
              "severity": "LOW",
              "confidence": 0.95,
              "evidence": "Error message: \"Cannot query field \\\"titel\\\" on type \\\"Album\\\". Did you mean \\\"title\\\"?\" (extensions.code: \"GRAPHQL_VALIDATION_FAILED\")",
              "explanation": "The GraphQL response returns a helpful suggestion that reveals the correct field name on the Album type: \"Did you mean \\\"title\\\"?\". This is a field_suggestions information-disclosure issue: an attacker can use such suggestions to enumerate schema details (field names and types) without authentication, making it easier to craft further targeted queries or perform automated schema discovery. The status code and validation error further confirm the server is returning detailed GraphQL validation feedback.\n\nAlthough this does not directly execute code or expose sensitive data, leaking schema details increases reconnaissance capability and should be treated as information disclosure. Mitigation: disable or suppress field-suggestion messages in production (turn off debug/verbose validation messages), return generic validation errors, and implement rate-limiting and authentication to reduce automated schema enumeration risk."
            }
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 1,
        "categories": [
          "Unknown"
        ],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [
          "Unknown",
          "Information Disclosure"
        ],
        "common_categories": [],
        "simple_unique_categories": [
          "Information Disclosure"
        ],
        "llm_unique_categories": [
          "Unknown"
        ],
        "agreement_score": 0.0,
        "consensus": "both_detected",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    }
  ],
  "detector_analysis": {
    "simple_detector": {
      "description": "Rule-based detector using predefined patterns",
      "strengths": [
        "Fast execution",
        "Consistent results",
        "Low resource usage"
      ],
      "weaknesses": [
        "Limited to known patterns",
        "May miss novel attacks",
        "False negatives possible"
      ]
    },
    "llm_detector": {
      "description": "AI-powered detector using language model analysis",
      "strengths": [
        "Contextual understanding",
        "Novel attack detection",
        "Detailed analysis"
      ],
      "weaknesses": [
        "Slower execution",
        "Resource intensive",
        "Potential false positives"
      ]
    }
  }
}