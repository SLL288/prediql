{
  "report_timestamp": "2025-10-03T05:11:49.344183",
  "api_url": "https://graphqlzero.almansi.me/api",
  "summary": {
    "total_comparisons": 10,
    "agreement_rate": 0.2,
    "simple_detections": 5,
    "llm_detections": 3,
    "consensus_breakdown": {
      "simple_only": 2,
      "both_detected": 3,
      "no_vulnerabilities": 5
    },
    "detection_ratio": 0.6,
    "potential_analysis": {
      "potential_agreement_rate": 0.0,
      "simple_potential_detections": 4,
      "llm_potential_detections": 0,
      "potential_consensus_breakdown": {
        "simple_potential_only": 4,
        "no_potential": 6
      },
      "potential_detection_ratio": 0.0
    }
  },
  "detailed_comparisons": [
    {
      "timestamp": "2025-10-03T05:10:38.321057",
      "node_name": "createAlbum_response_2",
      "response_status": 200,
      "response_body": {
        "data": {
          "createAlbum": {
            "id": "101",
            "title": "Robert'); DROP TABLE albums; --",
            "user": {
              "id": "1",
              "username": "Bret"
            }
          }
        }
      },
      "simple_detector": {
        "results": [
          {
            "detection_name": "SQL Injection (SQLi) Injection",
            "detection": "potential",
            "category": "Injection Attacks",
            "description": "SQL injection vulnerability detected - malicious SQL patterns found in response",
            "evidence": "Response status: 200, SQL patterns detected in response"
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 1,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": [
          "Injection Attacks"
        ]
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [
          "Injection Attacks"
        ],
        "common_categories": [],
        "simple_unique_categories": [
          "Injection Attacks"
        ],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "simple_only",
        "potential_analysis": {
          "total_potential_categories": [
            "Injection Attacks"
          ],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [
            "Injection Attacks"
          ],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "simple_potential_only"
        }
      }
    },
    {
      "timestamp": "2025-10-03T05:10:46.884291",
      "node_name": "createAlbum_response_3",
      "response_status": 200,
      "response_body": {
        "data": {
          "createAlbum": {
            "id": "101",
            "title": "<script>alert('xss-via-album')</script>",
            "user": {
              "id": "2",
              "name": "Ervin Howell"
            }
          }
        }
      },
      "simple_detector": {
        "results": [
          {
            "detection_name": "Cross-Site Scripting (XSS) Injection",
            "detection": "potential",
            "category": "Injection Attacks",
            "description": "XSS injection vulnerability detected - malicious script patterns found in response",
            "evidence": "Response status: 200, XSS patterns detected in response data"
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 1,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": [
          "Injection Attacks"
        ]
      },
      "llm_detector": {
        "results": [
          {
            "detection_name": "LLM: xss_injection",
            "detection": "vulnerable",
            "category": "Injection Attacks",
            "description": "The GraphQL response for createAlbum returns the title field containing raw script tags: <script>alert('xss-via-album')</script>. This indicates user-supplied content is being reflected (and likely stored) without HTML-encoding or sanitization. If a web client inserts this value into the DOM without proper output encoding (for example via innerHTML), the script will execute in the victim's browser — a classic XSS vector. Treat this as high risk because it allows arbitrary JavaScript execution in users' browsers. Recommended mitigations: perform server-side validation/sanitization, apply proper output encoding when rendering in HTML contexts, and adopt a strict Content Security Policy.",
            "evidence": "\"title\": \"<script>alert('xss-via-album')</script>\"",
            "confidence": 0.9,
            "llm_analysis": {
              "is_vulnerable": true,
              "vulnerability_type": "xss_injection",
              "severity": "HIGH",
              "confidence": 0.9,
              "evidence": "\"title\": \"<script>alert('xss-via-album')</script>\"",
              "explanation": "The GraphQL response for createAlbum returns the title field containing raw script tags: <script>alert('xss-via-album')</script>. This indicates user-supplied content is being reflected (and likely stored) without HTML-encoding or sanitization. If a web client inserts this value into the DOM without proper output encoding (for example via innerHTML), the script will execute in the victim's browser — a classic XSS vector. Treat this as high risk because it allows arbitrary JavaScript execution in users' browsers. Recommended mitigations: perform server-side validation/sanitization, apply proper output encoding when rendering in HTML contexts, and adopt a strict Content Security Policy."
            }
          }
        ],
        "count": 1,
        "vulnerable_count": 1,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [
          "Injection Attacks"
        ],
        "common_categories": [
          "Injection Attacks"
        ],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 1.0,
        "consensus": "both_detected",
        "potential_analysis": {
          "total_potential_categories": [
            "Injection Attacks"
          ],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [
            "Injection Attacks"
          ],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "simple_potential_only"
        }
      }
    },
    {
      "timestamp": "2025-10-03T05:10:52.919807",
      "node_name": "createAlbum_response_4",
      "response_status": 400,
      "response_body": {
        "errors": [
          {
            "message": "Field \"coverUrl\" is not defined by type \"CreateAlbumInput\".",
            "locations": [
              {
                "line": 3,
                "column": 47
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          }
        ]
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-03T05:11:05.514169",
      "node_name": "createAlbum_response_5",
      "response_status": 200,
      "response_body": {
        "data": {
          "createAlbum": {
            "id": "101",
            "title": "backup; cat /etc/passwd || true"
          }
        }
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-03T05:11:13.952437",
      "node_name": "createAlbum_response_6",
      "response_status": 200,
      "response_body": {
        "data": {
          "createAlbum": {
            "id": "101",
            "title": "../../etc/passwd"
          }
        }
      },
      "simple_detector": {
        "results": [
          {
            "detection_name": "Path Injection",
            "detection": "potential",
            "category": "Injection Attacks",
            "description": "Path injection vulnerability detected - file system access patterns found",
            "evidence": "Response status: 200, Path injection patterns detected in response"
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 1,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": [
          "Injection Attacks"
        ]
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [
          "Injection Attacks"
        ],
        "common_categories": [],
        "simple_unique_categories": [
          "Injection Attacks"
        ],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "simple_only",
        "potential_analysis": {
          "total_potential_categories": [
            "Injection Attacks"
          ],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [
            "Injection Attacks"
          ],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "simple_potential_only"
        }
      }
    },
    {
      "timestamp": "2025-10-03T05:11:20.895159",
      "node_name": "createAlbum_response_7",
      "response_status": 200,
      "response_body": {
        "data": {
          "createAlbum": {
            "id": "101",
            "title": "<div><img src=x onerror=alert('html-inject')></div>",
            "user": {
              "id": "6",
              "website": "ola.org"
            }
          }
        }
      },
      "simple_detector": {
        "results": [
          {
            "detection_name": "HTML Injection",
            "detection": "potential",
            "category": "Injection Attacks",
            "description": "HTML injection vulnerability detected - HTML markup found in response",
            "evidence": "Response status: 200, HTML patterns detected in response data"
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 1,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": [
          "Injection Attacks"
        ]
      },
      "llm_detector": {
        "results": [
          {
            "detection_name": "LLM: html_injection",
            "detection": "vulnerable",
            "category": "Injection Attacks",
            "description": "The GraphQL response returns unescaped HTML in the 'title' field: <div><img src=x onerror=alert('html-inject')></div>. The payload includes an onerror JavaScript handler which will execute if the value is inserted into a DOM context unsafely (for example via innerHTML or any client-side HTML injection). This is a clear HTML injection that can lead to XSS when rendered in a browser. Exploitation depends on how clients render the field, but the API should not return untrusted markup unencoded. Recommend encoding/sanitizing output, validating/stripping HTML on input, and applying CSP and safe rendering practices (e.g., textContent rather than innerHTML) to mitigate.",
            "evidence": "\"title\": \"<div><img src=x onerror=alert('html-inject')></div>\"",
            "confidence": 0.9,
            "llm_analysis": {
              "is_vulnerable": true,
              "vulnerability_type": "html_injection",
              "severity": "HIGH",
              "confidence": 0.9,
              "evidence": "\"title\": \"<div><img src=x onerror=alert('html-inject')></div>\"",
              "explanation": "The GraphQL response returns unescaped HTML in the 'title' field: <div><img src=x onerror=alert('html-inject')></div>. The payload includes an onerror JavaScript handler which will execute if the value is inserted into a DOM context unsafely (for example via innerHTML or any client-side HTML injection). This is a clear HTML injection that can lead to XSS when rendered in a browser. Exploitation depends on how clients render the field, but the API should not return untrusted markup unencoded. Recommend encoding/sanitizing output, validating/stripping HTML on input, and applying CSP and safe rendering practices (e.g., textContent rather than innerHTML) to mitigate."
            }
          }
        ],
        "count": 1,
        "vulnerable_count": 1,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [
          "Injection Attacks"
        ],
        "common_categories": [
          "Injection Attacks"
        ],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 1.0,
        "consensus": "both_detected",
        "potential_analysis": {
          "total_potential_categories": [
            "Injection Attacks"
          ],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [
            "Injection Attacks"
          ],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "simple_potential_only"
        }
      }
    },
    {
      "timestamp": "2025-10-03T05:11:26.903043",
      "node_name": "createAlbum_response_8",
      "response_status": null,
      "response_body": null,
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-03T05:11:32.136829",
      "node_name": "createAlbum_response_9",
      "response_status": null,
      "response_body": null,
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-03T05:11:42.013339",
      "node_name": "createAlbum_response_10",
      "response_status": 200,
      "response_body": {
        "data": {
          "createAlbum": {
            "id": "101",
            "title": "IntrospectProbe",
            "__typename": "Album",
            "user": {
              "id": "8",
              "__typename": "User",
              "name": "Nicholas Runolfsdottir V"
            }
          }
        }
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-03T05:11:49.341607",
      "node_name": "createAlbum_response_11",
      "response_status": 400,
      "response_body": {
        "errors": [
          {
            "message": "Cannot query field \"titl\" on type \"Album\". Did you mean \"title\"?",
            "locations": [
              {
                "line": 5,
                "column": 5
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          },
          {
            "message": "Cannot query field \"usre\" on type \"Album\". Did you mean \"user\"?",
            "locations": [
              {
                "line": 6,
                "column": 5
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          }
        ]
      },
      "simple_detector": {
        "results": [
          {
            "detection_name": "Field Suggestions Enabled",
            "detection": "vulnerable",
            "category": "Information Disclosure",
            "description": "Field suggestions enabled - potential information disclosure",
            "evidence": "Found \"did you mean\" in error message - field suggestions are enabled"
          }
        ],
        "count": 1,
        "vulnerable_count": 1,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [
          "Information Disclosure"
        ],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [
          {
            "detection_name": "LLM: field_suggestions",
            "detection": "safe",
            "category": "Unknown",
            "description": "The GraphQL response returns corrective suggestions for invalid field names (\"Did you mean 'title'?\" and \"Did you mean 'user'?\"). This leaks schema details (actual field names) which can aid attackers in enumerating the API schema and crafting targeted queries. While this does not itself execute code or return sensitive data, it is an information disclosure issue. Recommended mitigations: disable or sanitize suggestion messages in errors (e.g., disable detailed debug/errors in production, implement a custom formatError to return generic messages), restrict introspection in sensitive environments, and log detailed errors server-side rather than returning them to clients.",
            "evidence": "errors[0].message: \"Cannot query field \\\"titl\\\" on type \\\"Album\\\". Did you mean \\\"title\\\"?\"; errors[1].message: \"Cannot query field \\\"usre\\\" on type \\\"Album\\\". Did you mean \\\"user\\\"?\"",
            "confidence": 0.95,
            "llm_analysis": {
              "is_vulnerable": true,
              "vulnerability_type": "field_suggestions",
              "severity": "LOW",
              "confidence": 0.95,
              "evidence": "errors[0].message: \"Cannot query field \\\"titl\\\" on type \\\"Album\\\". Did you mean \\\"title\\\"?\"; errors[1].message: \"Cannot query field \\\"usre\\\" on type \\\"Album\\\". Did you mean \\\"user\\\"?\"",
              "explanation": "The GraphQL response returns corrective suggestions for invalid field names (\"Did you mean 'title'?\" and \"Did you mean 'user'?\"). This leaks schema details (actual field names) which can aid attackers in enumerating the API schema and crafting targeted queries. While this does not itself execute code or return sensitive data, it is an information disclosure issue. Recommended mitigations: disable or sanitize suggestion messages in errors (e.g., disable detailed debug/errors in production, implement a custom formatError to return generic messages), restrict introspection in sensitive environments, and log detailed errors server-side rather than returning them to clients."
            }
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 1,
        "categories": [
          "Unknown"
        ],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [
          "Information Disclosure",
          "Unknown"
        ],
        "common_categories": [],
        "simple_unique_categories": [
          "Information Disclosure"
        ],
        "llm_unique_categories": [
          "Unknown"
        ],
        "agreement_score": 0.0,
        "consensus": "both_detected",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    }
  ],
  "detector_analysis": {
    "simple_detector": {
      "description": "Rule-based detector using predefined patterns",
      "strengths": [
        "Fast execution",
        "Consistent results",
        "Low resource usage"
      ],
      "weaknesses": [
        "Limited to known patterns",
        "May miss novel attacks",
        "False negatives possible"
      ]
    },
    "llm_detector": {
      "description": "AI-powered detector using language model analysis",
      "strengths": [
        "Contextual understanding",
        "Novel attack detection",
        "Detailed analysis"
      ],
      "weaknesses": [
        "Slower execution",
        "Resource intensive",
        "Potential false positives"
      ]
    }
  }
}