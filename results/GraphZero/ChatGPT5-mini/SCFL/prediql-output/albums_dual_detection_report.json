{
  "report_timestamp": "2025-10-05T20:47:10.338855",
  "api_url": "https://graphqlzero.almansi.me/api",
  "summary": {
    "total_comparisons": 10,
    "agreement_rate": 0.0,
    "simple_detections": 1,
    "llm_detections": 4,
    "consensus_breakdown": {
      "no_vulnerabilities": 6,
      "llm_only": 3,
      "both_detected": 1
    },
    "detection_ratio": 4.0,
    "potential_analysis": {
      "potential_agreement_rate": 0.0,
      "simple_potential_detections": 0,
      "llm_potential_detections": 2,
      "potential_consensus_breakdown": {
        "no_potential": 8,
        "llm_potential_only": 2
      },
      "potential_detection_ratio": Infinity
    }
  },
  "detailed_comparisons": [
    {
      "timestamp": "2025-10-05T20:45:27.459391",
      "node_name": "albums_response_2",
      "response_status": 400,
      "response_body": {
        "errors": [
          {
            "message": "Expected value of type \"SearchOptions\", found \"1' OR '1'='1\".",
            "locations": [
              {
                "line": 3,
                "column": 28
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          }
        ]
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-05T20:45:40.000405",
      "node_name": "albums_response_3",
      "response_status": 400,
      "response_body": {
        "errors": [
          {
            "message": "Expected value of type \"SearchOptions\", found \"<script>alert('xss')</script>\".",
            "locations": [
              {
                "line": 3,
                "column": 28
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          }
        ]
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [
          {
            "detection_name": "LLM: xss_injection",
            "detection": "potential",
            "category": "Injection Attacks",
            "description": "The GraphQL error message reflects the exact submitted payload including a <script> tag: the server returned the user-supplied string \"<script>alert('xss')</script>\" inside the JSON error message. This is clear reflection of unsanitized input. If a client (browser or web UI) inserts this error message into a page without proper escaping/encoding, it can lead to reflected XSS. The risk depends on how the error text is rendered on the client, but the server-side behavior of echoing raw input in error messages is unsafe. Mitigations: do not include raw user input in error messages, or ensure output is properly escaped/encoded before rendering in HTML (prefer returning sanitized or generic error messages).",
            "evidence": "errors[0].message: \"Expected value of type \\\"SearchOptions\\\", found \\\"<script>alert('xss')</script>\\\".\"",
            "confidence": 0.8,
            "llm_analysis": {
              "is_vulnerable": true,
              "vulnerability_type": "xss_injection",
              "severity": "MEDIUM",
              "confidence": 0.8,
              "evidence": "errors[0].message: \"Expected value of type \\\"SearchOptions\\\", found \\\"<script>alert('xss')</script>\\\".\"",
              "explanation": "The GraphQL error message reflects the exact submitted payload including a <script> tag: the server returned the user-supplied string \"<script>alert('xss')</script>\" inside the JSON error message. This is clear reflection of unsanitized input. If a client (browser or web UI) inserts this error message into a page without proper escaping/encoding, it can lead to reflected XSS. The risk depends on how the error text is rendered on the client, but the server-side behavior of echoing raw input in error messages is unsafe. Mitigations: do not include raw user input in error messages, or ensure output is properly escaped/encoded before rendering in HTML (prefer returning sanitized or generic error messages)."
            }
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 1,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": [
          "Injection Attacks"
        ]
      },
      "comparison": {
        "total_categories": [
          "Injection Attacks"
        ],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [
          "Injection Attacks"
        ],
        "agreement_score": 0.0,
        "consensus": "llm_only",
        "potential_analysis": {
          "total_potential_categories": [
            "Injection Attacks"
          ],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [
            "Injection Attacks"
          ],
          "potential_agreement_score": 0.0,
          "potential_consensus": "llm_potential_only"
        }
      }
    },
    {
      "timestamp": "2025-10-05T20:45:50.809119",
      "node_name": "albums_response_4",
      "response_status": 400,
      "response_body": {
        "errors": [
          {
            "message": "Expected value of type \"SearchOptions\", found \"http://169.254.169.254/latest/meta-data/iam/security-credentials/\".",
            "locations": [
              {
                "line": 3,
                "column": 28
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          }
        ]
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-05T20:46:00.493847",
      "node_name": "albums_response_5",
      "response_status": 400,
      "response_body": {
        "errors": [
          {
            "message": "Expected value of type \"SearchOptions\", found \"test || cat /etc/passwd\".",
            "locations": [
              {
                "line": 3,
                "column": 28
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          }
        ]
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-05T20:46:11.273155",
      "node_name": "albums_response_6",
      "response_status": 400,
      "response_body": {
        "errors": [
          {
            "message": "Expected value of type \"SearchOptions\", found \"../../../../etc/passwd\".",
            "locations": [
              {
                "line": 3,
                "column": 28
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          }
        ]
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-05T20:46:26.234826",
      "node_name": "albums_response_7",
      "response_status": 400,
      "response_body": {
        "errors": [
          {
            "message": "Expected value of type \"SearchOptions\", found \"<div style='color:red'>INJECTED</div>\".",
            "locations": [
              {
                "line": 3,
                "column": 28
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          }
        ]
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [
          {
            "detection_name": "LLM: html_injection",
            "detection": "potential",
            "category": "Injection Attacks",
            "description": "The GraphQL error message reflects raw HTML markup that was supplied as input: the response contains the string \"<div style='color:red'>INJECTED</div>\" unescaped. This indicates the server is echoing user-controlled content into the response without sanitization or encoding. If a client (for example a web UI) inserts these error messages into the DOM without proper escaping, this reflection can lead to HTML injection / reflected XSS. The risk depends on how clients render GraphQL error messages, but the presence of unescaped HTML in API output is a clear unsafe behavior. Recommended mitigations: ensure the server does not embed raw user input in responses or properly escape/encode it, and ensure any consumer renders error text as plain text (not as HTML).",
            "evidence": "Error message returned: \"Expected value of type \\\"SearchOptions\\\", found \\\"<div style='color:red'>INJECTED</div>\\\".\"",
            "confidence": 0.8,
            "llm_analysis": {
              "is_vulnerable": true,
              "vulnerability_type": "html_injection",
              "severity": "MEDIUM",
              "confidence": 0.8,
              "evidence": "Error message returned: \"Expected value of type \\\"SearchOptions\\\", found \\\"<div style='color:red'>INJECTED</div>\\\".\"",
              "explanation": "The GraphQL error message reflects raw HTML markup that was supplied as input: the response contains the string \"<div style='color:red'>INJECTED</div>\" unescaped. This indicates the server is echoing user-controlled content into the response without sanitization or encoding. If a client (for example a web UI) inserts these error messages into the DOM without proper escaping, this reflection can lead to HTML injection / reflected XSS. The risk depends on how clients render GraphQL error messages, but the presence of unescaped HTML in API output is a clear unsafe behavior. Recommended mitigations: ensure the server does not embed raw user input in responses or properly escape/encode it, and ensure any consumer renders error text as plain text (not as HTML)."
            }
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 1,
        "safe_count": 0,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": [
          "Injection Attacks"
        ]
      },
      "comparison": {
        "total_categories": [
          "Injection Attacks"
        ],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [
          "Injection Attacks"
        ],
        "agreement_score": 0.0,
        "consensus": "llm_only",
        "potential_analysis": {
          "total_potential_categories": [
            "Injection Attacks"
          ],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [
            "Injection Attacks"
          ],
          "potential_agreement_score": 0.0,
          "potential_consensus": "llm_potential_only"
        }
      }
    },
    {
      "timestamp": "2025-10-05T20:46:34.235187",
      "node_name": "albums_response_8",
      "response_status": null,
      "response_body": null,
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-05T20:46:47.992927",
      "node_name": "albums_response_9",
      "response_status": null,
      "response_body": null,
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [],
        "agreement_score": 0.0,
        "consensus": "no_vulnerabilities",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-05T20:47:02.001935",
      "node_name": "albums_response_10",
      "response_status": 400,
      "response_body": {
        "errors": [
          {
            "message": "Cannot query field \"types\" on type \"AlbumsPage\".",
            "locations": [
              {
                "line": 4,
                "column": 5
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          }
        ]
      },
      "simple_detector": {
        "results": [],
        "count": 0,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [
          {
            "detection_name": "LLM: Introspection (schema disclosure via error messages)",
            "detection": "safe",
            "category": "Injection Attacks",
            "description": "The GraphQL response discloses internal schema details (the type name 'AlbumsPage' and that a 'types' field was queried). Even though this is not a direct __schema or __type introspection response, returning validation errors that include type/field names effectively allows an attacker to enumerate the schema and infer structure. This is an information-disclosure form of GraphQL introspection. Recommended mitigations: suppress detailed schema validation errors in production, enable error masking/logging only server-side, and restrict or disable GraphQL introspection in public environments or enforce authentication/authorization checks for schema exploration.",
            "evidence": "GraphQL error returned: \"Cannot query field \\\"types\\\" on type \\\"AlbumsPage\\\".\"",
            "confidence": 0.7,
            "llm_analysis": {
              "is_vulnerable": true,
              "vulnerability_type": "Introspection (schema disclosure via error messages)",
              "severity": "LOW",
              "confidence": 0.7,
              "evidence": "GraphQL error returned: \"Cannot query field \\\"types\\\" on type \\\"AlbumsPage\\\".\"",
              "explanation": "The GraphQL response discloses internal schema details (the type name 'AlbumsPage' and that a 'types' field was queried). Even though this is not a direct __schema or __type introspection response, returning validation errors that include type/field names effectively allows an attacker to enumerate the schema and infer structure. This is an information-disclosure form of GraphQL introspection. Recommended mitigations: suppress detailed schema validation errors in production, enable error masking/logging only server-side, and restrict or disable GraphQL introspection in public environments or enforce authentication/authorization checks for schema exploration."
            }
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 1,
        "categories": [
          "Injection Attacks"
        ],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [
          "Injection Attacks"
        ],
        "common_categories": [],
        "simple_unique_categories": [],
        "llm_unique_categories": [
          "Injection Attacks"
        ],
        "agreement_score": 0.0,
        "consensus": "llm_only",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    },
    {
      "timestamp": "2025-10-05T20:47:10.336469",
      "node_name": "albums_response_11",
      "response_status": 400,
      "response_body": {
        "errors": [
          {
            "message": "Cannot query field \"titl\" on type \"Album\". Did you mean \"title\"?",
            "locations": [
              {
                "line": 6,
                "column": 7
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          },
          {
            "message": "Cannot query field \"usernme\" on type \"User\". Did you mean \"username\"?",
            "locations": [
              {
                "line": 9,
                "column": 9
              }
            ],
            "extensions": {
              "code": "GRAPHQL_VALIDATION_FAILED"
            }
          }
        ]
      },
      "simple_detector": {
        "results": [
          {
            "detection_name": "Field Suggestions Enabled",
            "detection": "vulnerable",
            "category": "Information Disclosure",
            "description": "Field suggestions enabled - potential information disclosure",
            "evidence": "Found \"did you mean\" in error message - field suggestions are enabled"
          }
        ],
        "count": 1,
        "vulnerable_count": 1,
        "potential_count": 0,
        "safe_count": 0,
        "categories": [
          "Information Disclosure"
        ],
        "potential_categories": []
      },
      "llm_detector": {
        "results": [
          {
            "detection_name": "LLM: field_suggestions",
            "detection": "safe",
            "category": "Unknown",
            "description": "The GraphQL response returns automatic field suggestions in error messages that reveal valid schema field names (e.g., suggesting \"title\" for Album and \"username\" for User). This is an information disclosure (field_suggestions) vulnerability: an attacker can use these suggestions to enumerate the schema and discover valid field names, which can aid in crafting further queries or attacks. While this does not directly expose sensitive data or indicate injection, it leaks internal API structure. Recommended remediation: disable or sanitize suggestion messages in production (configure the GraphQL library to not return 'did you mean' hints), return generic validation errors, and implement strict error handling/logging that does not reveal schema details.",
            "evidence": "Error messages: \"Cannot query field \\\"titl\\\" on type \\\"Album\\\". Did you mean \\\"title\\\"?\" and \"Cannot query field \\\"usernme\\\" on type \\\"User\\\". Did you mean \\\"username\\\"?\"",
            "confidence": 0.9,
            "llm_analysis": {
              "is_vulnerable": true,
              "vulnerability_type": "field_suggestions",
              "severity": "LOW",
              "confidence": 0.9,
              "evidence": "Error messages: \"Cannot query field \\\"titl\\\" on type \\\"Album\\\". Did you mean \\\"title\\\"?\" and \"Cannot query field \\\"usernme\\\" on type \\\"User\\\". Did you mean \\\"username\\\"?\"",
              "explanation": "The GraphQL response returns automatic field suggestions in error messages that reveal valid schema field names (e.g., suggesting \"title\" for Album and \"username\" for User). This is an information disclosure (field_suggestions) vulnerability: an attacker can use these suggestions to enumerate the schema and discover valid field names, which can aid in crafting further queries or attacks. While this does not directly expose sensitive data or indicate injection, it leaks internal API structure. Recommended remediation: disable or sanitize suggestion messages in production (configure the GraphQL library to not return 'did you mean' hints), return generic validation errors, and implement strict error handling/logging that does not reveal schema details."
            }
          }
        ],
        "count": 1,
        "vulnerable_count": 0,
        "potential_count": 0,
        "safe_count": 1,
        "categories": [
          "Unknown"
        ],
        "potential_categories": []
      },
      "comparison": {
        "total_categories": [
          "Information Disclosure",
          "Unknown"
        ],
        "common_categories": [],
        "simple_unique_categories": [
          "Information Disclosure"
        ],
        "llm_unique_categories": [
          "Unknown"
        ],
        "agreement_score": 0.0,
        "consensus": "both_detected",
        "potential_analysis": {
          "total_potential_categories": [],
          "common_potential_categories": [],
          "simple_unique_potential_categories": [],
          "llm_unique_potential_categories": [],
          "potential_agreement_score": 0.0,
          "potential_consensus": "no_potential"
        }
      }
    }
  ],
  "detector_analysis": {
    "simple_detector": {
      "description": "Rule-based detector using predefined patterns",
      "strengths": [
        "Fast execution",
        "Consistent results",
        "Low resource usage"
      ],
      "weaknesses": [
        "Limited to known patterns",
        "May miss novel attacks",
        "False negatives possible"
      ]
    },
    "llm_detector": {
      "description": "AI-powered detector using language model analysis",
      "strengths": [
        "Contextual understanding",
        "Novel attack detection",
        "Detailed analysis"
      ],
      "weaknesses": [
        "Slower execution",
        "Resource intensive",
        "Potential false positives"
      ]
    }
  }
}